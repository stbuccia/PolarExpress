{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of the project is to learn the mapping from polar coordinates to a a discrete 10x10 grid of cells in the plane, using a neural network. \n",
        "\n",
        "The supervised dataset is given to you in the form of a generator (to be considered as a black box).\n",
        "\n",
        "The model must achieve an accuracy of 95%, and it will be evaluated in a way **inversely proportional to the number of its parameters: the smaller, the better.**\n",
        "\n",
        "**WARNING**: Any solution taking advantage of meta-knowledge about the generator will be automatically rejected."
      ],
      "metadata": {
        "id": "Zw_326KLT9dF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynz-4_4cFmbJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the generator. It returns triples of the form ((theta,rho),out) where (theta,rho) are the polar coordinates of a point in the first quadrant of the plane, and out is a 10x10 map with \"1\" in the cell corresponding to the point position, and \"0\" everywhere else.\n",
        "\n",
        "By setting flat=True, the resulting map is flattened into a vector with a single dimension 100. You can use this variant, if you wish. "
      ],
      "metadata": {
        "id": "iA01pkKbUt7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polar_generator(batchsize,grid=(10,10),noise=.002,flat=False):\n",
        "  while True:\n",
        "    x = np.random.rand(batchsize)\n",
        "    y = np.random.rand(batchsize)\n",
        "    out = np.zeros((batchsize,grid[0],grid[1]))\n",
        "    xc = (x*grid[0]).astype(int)\n",
        "    yc = (y*grid[1]).astype(int)\n",
        "    for b in range(batchsize):\n",
        "      out[b,xc[b],yc[b]] = 1\n",
        "    #compute rho and theta and add some noise\n",
        "    rho = np.sqrt(x**2+y**2) + np.random.normal(scale=noise)\n",
        "    theta = np.arctan(y/np.maximum(x,.00001)) + np.random.normal(scale=noise)\n",
        "    if flat:\n",
        "      out = np.reshape(out,(batchsize,grid[0]*grid[1]))\n",
        "    yield ((theta,rho),out)"
      ],
      "metadata": {
        "id": "DsA1GqAeWAdo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create an instance of the generator on a grid with dimension 3x4"
      ],
      "metadata": {
        "id": "ZF-jlaqAWc2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g1,g2 = 3,4\n",
        "gen = polar_generator(1,grid=(g1,g2),noise=0.0)"
      ],
      "metadata": {
        "id": "Ov3rXaLVHDCT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's see a few samples."
      ],
      "metadata": {
        "id": "b4hntQtSWjPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(theta,rho),maps = next(gen)\n",
        "for i,map in enumerate(maps):\n",
        "  #let us compute the cartesian coordinates\n",
        "  x = np.cos(theta[i])*rho[i]\n",
        "  y = np.sin(theta[i])*rho[i]\n",
        "  print(\"x coordinate (row): {}\".format(int(x*g1)))\n",
        "  print(\"y coordinate (col): {}\".format(int(y*g2)))\n",
        "  print(\"map:\")\n",
        "  print(np.reshape(map,(g1,g2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM7R8ZZZHN7p",
        "outputId": "e67c454f-e7fa-44b7-d2d2-68ce9c884ec9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x coordinate (row): 1\n",
            "y coordinate (col): 3\n",
            "map:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: add noise to the generator, and check the effect on the \"ground truth\"."
      ],
      "metadata": {
        "id": "NTY5fu8Hg7RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What to deliver\n",
        "\n",
        "For the purposes of the project you are supposed to work with the **default 10x10 grid, and the default noise=.002**\n",
        "\n",
        "The generator must be treatead as a black box, do not tweak it, and do not exploit its semantics that is supposed to be unknown. You are allowed to work with the \"flat\" modality, if you prefer so.\n",
        "\n",
        "You need to:\n",
        "1.   define an accuracy function (take inspiration from the code of the previous cell)\n",
        "2.   define a neural network taking in input theta and rho, and returning out\n",
        "3. measure the network's accuracy that must be above 95% (accuracy must be evaluated over at least 20000 samples)\n",
        "4. tune the network trying to decrease as much as possible the numer of parameters, preserving an accuracy above 95%. Only your best network must be delivered.\n",
        "\n",
        "You must deliver a SINGLE notebook working on colab, containing the code of the network, its summary, the training history, the code for the accurary metric and its evaluation on the network.\n",
        "\n",
        "**N.B.** The accuracy must be above 95% but apart from that it does not influence the evaluation. You score will only depend on the number of parameters: the lower, the better.\n",
        "\n",
        "#Good work!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jj4akvA24maJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPmM-TvTfjoW"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "From this point starts my work."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensions of the matrix output are parameterized"
      ],
      "metadata": {
        "id": "z-OkmVqzOHYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g1,g2 = 10,10"
      ],
      "metadata": {
        "id": "AW5sej_wN0DJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hj0PHSCfYHu"
      },
      "source": [
        "## Accuracy Function\n",
        "\n",
        "In order to have a good precised accuracy, the evaluation takes 200000 validation samples from the generator, that returns the input (theta and rho) and the correct output tensors. The predicted cartesian coordinates are evaluated appling the NN on input tensors and, for each matrix of the output, and taking row and column positions of the value most similar to 1. Then is counted the correct predictions over the total.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IqxH7ZZnBQAp"
      },
      "outputs": [],
      "source": [
        "def accuracy(model):\n",
        "  gen = polar_generator(200000)\n",
        "  (theta,rho),maps_true = next(gen)\n",
        "\n",
        "\n",
        "  maps_pred = model.predict([np.array(theta), np.array(rho)])\n",
        "\n",
        "  correct_predictions = 0\n",
        "  total_predictions = len(maps_true)\n",
        "\n",
        "  for i,map_pred in enumerate(maps_pred):\n",
        "\n",
        "    (x_pred,y_pred) = np.unravel_index(map_pred.argmax(), map_pred.shape)\n",
        "\n",
        "    x= np.cos(theta[i])*rho[i]\n",
        "    y = np.sin(theta[i])*rho[i]\n",
        "    x_true = int(x*g1)\n",
        "    y_true = int(y*g2)\n",
        "\n",
        "    if x_true == x_pred and y_true == y_pred :\n",
        "      correct_predictions = correct_predictions + 1\n",
        "\n",
        "  return correct_predictions/total_predictions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddDS9YlZfdhZ"
      },
      "source": [
        "## Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definition"
      ],
      "metadata": {
        "id": "IsUMX-WsTizX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CF14fSOv4vmp"
      },
      "outputs": [],
      "source": [
        "input_layer_1 = Input(shape=(1,))\n",
        "input_layer_2 = Input(shape=(1,))\n",
        "\n",
        "\n",
        "x = Concatenate(axis=1)([input_layer_1, input_layer_2])\n",
        "\n",
        "hidden_layers_dims = [5,5]\n",
        "for i in range(0,len(hidden_layers_dims)):\n",
        "  x = Dense(hidden_layers_dims[i], activation='relu')(x)\n",
        "\n",
        "x = Dense(g1*g2, activation='softmax')(x)\n",
        "output_layer = Reshape((g1,g2))(x)\n",
        "\n",
        "model = Model([input_layer_1,input_layer_2],output_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xhd_JDCfwtk"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TZe0W0qW42tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e559908a-0ce0-4392-f076-584329ca552d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 2)            0           ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 5)            15          ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 5)            30          ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 100)          600         ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 10, 10)       0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 645\n",
            "Trainable params: 645\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "The NN takes about 20 minutes to be trained"
      ],
      "metadata": {
        "id": "mHZowzmPE1Lb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FdcU2v9r44eL"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',loss='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TFGPu-5E46L_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf996207-48f5-4fe7-874d-d977d6f8c8ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1000/1000 [==============================] - 16s 16ms/step - loss: 0.0095\n",
            "Epoch 2/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0071\n",
            "Epoch 3/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0052\n",
            "Epoch 4/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0042\n",
            "Epoch 5/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0036\n",
            "Epoch 6/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0032\n",
            "Epoch 7/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0030\n",
            "Epoch 8/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0028\n",
            "Epoch 9/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0026\n",
            "Epoch 10/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0025\n",
            "Epoch 11/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0023\n",
            "Epoch 12/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0023\n",
            "Epoch 13/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0022\n",
            "Epoch 14/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0021\n",
            "Epoch 15/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0020\n",
            "Epoch 16/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0020\n",
            "Epoch 17/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0019\n",
            "Epoch 18/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0019\n",
            "Epoch 19/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0019\n",
            "Epoch 20/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0018\n",
            "Epoch 21/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0018\n",
            "Epoch 22/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0018\n",
            "Epoch 23/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0017\n",
            "Epoch 24/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0017\n",
            "Epoch 25/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0017\n",
            "Epoch 26/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0016\n",
            "Epoch 27/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0016\n",
            "Epoch 28/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0016\n",
            "Epoch 29/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0016\n",
            "Epoch 30/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 0.0016\n",
            "Epoch 31/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0015\n",
            "Epoch 32/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0015\n",
            "Epoch 33/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0015\n",
            "Epoch 34/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0015\n",
            "Epoch 35/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0015\n",
            "Epoch 36/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0015\n",
            "Epoch 37/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 38/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 39/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 40/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 41/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 42/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 43/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 44/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0014\n",
            "Epoch 45/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 46/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 47/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 48/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 49/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 50/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 51/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 52/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 53/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 54/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0013\n",
            "Epoch 55/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 56/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 57/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 58/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 59/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 60/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 61/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 62/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 63/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 64/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 65/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 66/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 67/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 68/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0012\n",
            "Epoch 69/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 70/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 71/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 72/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 73/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 74/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 75/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 76/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 77/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 78/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 79/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 80/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 81/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 82/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 83/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 84/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 85/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 86/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 87/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 88/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0011\n",
            "Epoch 89/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 90/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 91/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 92/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 93/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 94/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 95/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 96/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 97/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 98/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 99/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 100/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 101/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 0.0010\n",
            "Epoch 102/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.9820e-04\n",
            "Epoch 103/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.9203e-04\n",
            "Epoch 104/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.8858e-04\n",
            "Epoch 105/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.8416e-04\n",
            "Epoch 106/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.7971e-04\n",
            "Epoch 107/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.7531e-04\n",
            "Epoch 108/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.7162e-04\n",
            "Epoch 109/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.7015e-04\n",
            "Epoch 110/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.6592e-04\n",
            "Epoch 111/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.5957e-04\n",
            "Epoch 112/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.5457e-04\n",
            "Epoch 113/150\n",
            "1000/1000 [==============================] - 7s 6ms/step - loss: 9.4492e-04\n",
            "Epoch 114/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.4325e-04\n",
            "Epoch 115/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.4077e-04\n",
            "Epoch 116/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.3559e-04\n",
            "Epoch 117/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.3578e-04\n",
            "Epoch 118/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.4035e-04\n",
            "Epoch 119/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 9.2951e-04\n",
            "Epoch 120/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.2571e-04\n",
            "Epoch 121/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.2389e-04\n",
            "Epoch 122/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.2267e-04\n",
            "Epoch 123/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 9.2149e-04\n",
            "Epoch 124/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.1748e-04\n",
            "Epoch 125/150\n",
            "1000/1000 [==============================] - 7s 6ms/step - loss: 9.1090e-04\n",
            "Epoch 126/150\n",
            "1000/1000 [==============================] - 6s 6ms/step - loss: 9.0391e-04\n",
            "Epoch 127/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.0849e-04\n",
            "Epoch 128/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.0463e-04\n",
            "Epoch 129/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 9.0428e-04\n",
            "Epoch 130/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.9621e-04\n",
            "Epoch 131/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.9875e-04\n",
            "Epoch 132/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.9331e-04\n",
            "Epoch 133/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.8720e-04\n",
            "Epoch 134/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.9167e-04\n",
            "Epoch 135/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.8868e-04\n",
            "Epoch 136/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.8282e-04\n",
            "Epoch 137/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.8174e-04\n",
            "Epoch 138/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.8197e-04\n",
            "Epoch 139/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.7828e-04\n",
            "Epoch 140/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.7641e-04\n",
            "Epoch 141/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.7085e-04\n",
            "Epoch 142/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.6511e-04\n",
            "Epoch 143/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.7663e-04\n",
            "Epoch 144/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.6713e-04\n",
            "Epoch 145/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.6677e-04\n",
            "Epoch 146/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.6329e-04\n",
            "Epoch 147/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.6338e-04\n",
            "Epoch 148/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.5565e-04\n",
            "Epoch 149/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.5655e-04\n",
            "Epoch 150/150\n",
            "1000/1000 [==============================] - 7s 7ms/step - loss: 8.5627e-04\n"
          ]
        }
      ],
      "source": [
        "training_samples_number = 4000\n",
        "\n",
        "history = model.fit(polar_generator(training_samples_number), batch_size=128, steps_per_epoch=1000, epochs=150)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training History Representation\n",
        "\n",
        "The chart represent the loss function reduction over the epochs in training"
      ],
      "metadata": {
        "id": "4llszNGJVAt5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "7d2mmoQxeKJV",
        "outputId": "592e3fb9-bcbb-4173-e5cf-127661c86c85"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGGheARLFIcz"
      },
      "source": [
        "## Accuracy Evaluation\n",
        "\n",
        "The accuracy of the model is over 95%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ft_gAo9149l-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0407af02-887f-4c1f-9f20-624a5500a4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6250/6250 [==============================] - 5s 788us/step\n",
            "0.96585\n"
          ]
        }
      ],
      "source": [
        "print(accuracy(model))"
      ]
    }
  ]
}